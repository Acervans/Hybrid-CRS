{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLamaIndex Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex training and practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def display_markdown(content, clear=True):\n",
    "    display(\n",
    "        Markdown(\n",
    "            content.replace('\\[', '$$').replace('\\]', '$$').replace(\n",
    "                '\\(', '$').replace('\\)', '$')\n",
    "        ),\n",
    "        clear=clear\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Settings.llm = Ollama(model=\"qwen2.5:3b\", request_timeout=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Matrix multiplication is a fundamental operation in linear algebra where you take two matrices and produce another matrix as their result. Hereâ€™s a simple breakdown of how it works:\n",
       "\n",
       "### Understanding Matrices\n",
       "\n",
       "First, let's understand what a matrix is: A matrix is essentially a rectangular array of numbers (or other mathematical objects). The size or dimension of the matrix is defined by its number of rows and columns.\n",
       "\n",
       "For example:\n",
       "- A 2x3 matrix has two rows and three columns.\n",
       "- A 3x2 matrix has three rows and two columns.\n",
       "\n",
       "### Matrix Multiplication\n",
       "\n",
       "Matrix multiplication involves taking two matrices and producing another matrix. To multiply two matrices $ A $ and $ B $, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The result will have the same number of rows as the first matrix and the same number of columns as the second.\n",
       "\n",
       "Let's denote:\n",
       "- Matrix $ A $ is an $ m \\times n $ matrix (with $ m $ rows and $ n $ columns).\n",
       "- Matrix $ B $ is a $ p \\times q $ matrix (with $ p $ rows and $ q $ columns).\n",
       "\n",
       "The resulting matrix $ C $, which will have dimensions $ m \\times q $, can be calculated as follows:\n",
       "\n",
       "1. **Identify the elements**: \n",
       "   - The element in the $ i $-th row and $ j $-th column of the resulting matrix $ C $ is found by multiplying each element of the $ i $-th row of matrix $ A $ with the corresponding element from the $ j $-th column of matrix $ B $, and then summing these products.\n",
       "\n",
       "2. **Calculate elements**:\n",
       "   - Specifically, if we want to calculate the element at position (i, j) in the resulting matrix $ C $:\n",
       "     $$\n",
       "     c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
       "     $$\n",
       "   - Here, $ a_{ik} $ is an entry from row $ i $ and column $ k $ of matrix $ A $, and $ b_{kj} $ is the corresponding element in matrix $ B $.\n",
       "\n",
       "### Example\n",
       "\n",
       "Let's take two matrices for example:\n",
       "\n",
       "- Matrix $ A = \\begin{bmatrix}\n",
       "1 & 2 \\\\\n",
       "3 & 4\n",
       "\\end{bmatrix} $\n",
       "- Matrix $ B = \\begin{bmatrix}\n",
       "5 & 6 \\\\\n",
       "7 & 8\n",
       "\\end{bmatrix} $\n",
       "\n",
       "To find the product $ C = A \\times B $:\n",
       "\n",
       "The element at position (1,1) of matrix $ C $ is:\n",
       "$$ c_{11} = (1*5) + (2*7) = 5 + 14 = 19 $$\n",
       "\n",
       "The element at position (1,2) of matrix $ C $ is:\n",
       "$$ c_{12} = (1*6) + (2*8) = 6 + 16 = 22 $$\n",
       "\n",
       "The element at position (2,1) of matrix $ C $ is:\n",
       "$$ c_{21} = (3*5) + (4*7) = 15 + 28 = 43 $$\n",
       "\n",
       "The element at position (2,2) of matrix $ C $ is:\n",
       "$$ c_{22} = (3*6) + (4*8) = 18 + 32 = 50 $$\n",
       "\n",
       "Thus, the resulting matrix $ C $ would be:\n",
       "$$\n",
       "C = \\begin{bmatrix}\n",
       "19 & 22 \\\\\n",
       "43 & 50\n",
       "\\end{bmatrix}\n",
       "$$\n",
       "\n",
       "### Summary\n",
       "\n",
       "In essence, to multiply two matrices $ A \\times B $:\n",
       "- The number of columns in the first matrix $ A $ must match the number of rows in the second matrix $ B $.\n",
       "- Each element in the resulting matrix is computed by taking a row from the first matrix and multiplying it with a column from the second matrix.\n",
       "- Sum these products to get each corresponding entry in the resulting matrix.\n",
       "\n",
       "Matrix multiplication can be used for various applications, including transformations (such as scaling or rotating images), solving systems of linear equations, and many other areas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Explain matrix multiplication simply.\"\n",
    "\n",
    "gen = llm.stream_complete(prompt)\n",
    "\n",
    "for response in gen:\n",
    "    display_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event, Context\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput\n",
    "\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class ToolCallOutputEvent(Event):\n",
    "    tool_call: ToolSelection\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "class FunctionCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or Ollama(model='qwen2.5:3b')\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(self, ctx: Context, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        \n",
    "        next_ev = InputEvent(input=chat_history)\n",
    "        ctx.write_event_to_stream(next_ev)\n",
    "        return next_ev\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history, allow_parallel_tool_calls=False\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=f'LLM response: {response}'))\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            next_ev = StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*self.sources]}\n",
    "            )\n",
    "        else:\n",
    "            next_ev = ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "        ctx.write_event_to_stream(next_ev)\n",
    "        return next_ev\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(self, ctx: Context, ev: ToolCallEvent) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            ctx.write_event_to_stream(ToolCallOutputEvent(tool_call=tool_call, output=tool_msgs[-1].content))\n",
    "\n",
    "        for msg in tool_msgs:\n",
    "            self.memory.put(msg)\n",
    "\n",
    "        chat_history = self.memory.get()\n",
    "\n",
    "        next_ev = InputEvent(input=chat_history)\n",
    "        ctx.write_event_to_stream(next_ev)\n",
    "        return next_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to add two numbers.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to multiply two numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(add),\n",
    "    FunctionTool.from_defaults(multiply),\n",
    "]\n",
    "\n",
    "agent = FunctionCallingAgent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    timeout=120,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "ret = await agent.run(input=\"What is (29485 + 193482) * 999?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The result of (29,485 + 193,482) * 999 is 222,744,033."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(ret['response'].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is (29485 + 193482) * 999?', additional_kwargs={})]\n",
      "msg='LLM response: assistant: '\n",
      "tool_calls=[ToolSelection(tool_id='add', tool_name='add', tool_kwargs={'x': 29485, 'y': 193482})]\n",
      "tool_call=ToolSelection(tool_id='add', tool_name='add', tool_kwargs={'x': 29485, 'y': 193482}) output='222967'\n",
      "input=[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is (29485 + 193482) * 999?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='', additional_kwargs={'tool_calls': [{'function': {'name': 'add', 'arguments': {'x': 29485, 'y': 193482}}}]}), ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='222967', additional_kwargs={'tool_call_id': 'add', 'name': 'add'})]\n",
      "msg='LLM response: assistant: '\n",
      "tool_calls=[ToolSelection(tool_id='multiply', tool_name='multiply', tool_kwargs={'x': 222967, 'y': 999})]\n",
      "tool_call=ToolSelection(tool_id='multiply', tool_name='multiply', tool_kwargs={'x': 222967, 'y': 999}) output='222744033'\n",
      "input=[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is (29485 + 193482) * 999?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='', additional_kwargs={'tool_calls': [{'function': {'name': 'add', 'arguments': {'x': 29485, 'y': 193482}}}]}), ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='222967', additional_kwargs={'tool_call_id': 'add', 'name': 'add'}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='', additional_kwargs={'tool_calls': [{'function': {'name': 'multiply', 'arguments': {'x': 222967, 'y': 999}}}]}), ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='222744033', additional_kwargs={'tool_call_id': 'multiply', 'name': 'multiply'})]\n",
      "msg='LLM response: assistant: The result of (29485 + 193482) * 999 is 222744033.'\n",
      "result={'response': ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The result of (29485 + 193482) * 999 is 222744033.', additional_kwargs={'tool_calls': []}), raw={'model': 'qwen2.5:3b', 'created_at': '2024-12-04T15:35:20.624461607Z', 'message': {'role': 'assistant', 'content': 'The result of (29485 + 193482) * 999 is 222744033.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 741822068, 'load_duration': 14595653, 'prompt_eval_count': 392, 'prompt_eval_duration': 26000000, 'eval_count': 36, 'eval_duration': 645000000, 'usage': {'prompt_tokens': 392, 'completion_tokens': 36, 'total_tokens': 428}}, delta=None, logprobs=None, additional_kwargs={}), 'sources': [ToolOutput(content='222967', tool_name='add', raw_input={'args': (), 'kwargs': {'x': 29485, 'y': 193482}}, raw_output=222967, is_error=False), ToolOutput(content='222744033', tool_name='multiply', raw_input={'args': (), 'kwargs': {'x': 222967, 'y': 999}}, raw_output=222744033, is_error=False)]}\n",
      "Final result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of (29485 + 193482) * 999 is 222744033."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "async def main():\n",
    "    agent = FunctionCallingAgent(\n",
    "        llm=llm,\n",
    "        tools=tools,\n",
    "        timeout=120,\n",
    "        verbose=False\n",
    "    )\n",
    "    handler = agent.run(input=\"What is (29485 + 193482) * 999?\")\n",
    "\n",
    "    async for ev in handler.stream_events():\n",
    "        print(ev)\n",
    "\n",
    "    final_result = await handler\n",
    "    print(\"Final result\")\n",
    "    display_markdown(final_result['response'].message.content, clear=False)\n",
    "\n",
    "    draw_all_possible_flows(agent, filename=\"streaming_workflow.html\")\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid-crs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
